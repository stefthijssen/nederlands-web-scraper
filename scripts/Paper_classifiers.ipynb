{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_f = open('../data/nl.json')\n",
    "en_f = open('../data/en.json')\n",
    "nl_data = json.load(nl_f)\n",
    "en_data = json.load(en_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_tokenized = []\n",
    "excluded_keywords = ['www', 'index', 'html', 'htm', 'html', 'http', 'https']\n",
    "nl_tlds = ['.nl/', '.be/', '.su/', '.aw/', '.sx/', '.cw/']\n",
    "other_tlds = ['.com/', '.net/', '.org/']\n",
    "\n",
    "for element in nl_data[0:10000]:\n",
    "    url_element = element['siteUrl'] + ('' if element['siteUrl'].endswith('/') else '/')\n",
    "    split = [word for word in re.split('[^a-zA-Z]', url_element) \n",
    "        if len(word) >= 2 and word not in excluded_keywords]\n",
    "    split_sentence = ' '.join(split)\n",
    "    custom_feat =[\n",
    "        1 if any(tld in url_element for tld in nl_tlds) else 0,\n",
    "        1 if any(tld in url_element for tld in other_tlds) else 0\n",
    "    ]\n",
    "    urls_tokenized.append([split_sentence, custom_feat, url_element, 1])\n",
    "\n",
    "for element in en_data[0:10000]:\n",
    "    url_element = element['siteUrl'] + ('' if element['siteUrl'].endswith('/') else '/')\n",
    "    split = [word for word in re.split('[^a-zA-Z]', url_element) \n",
    "        if len(word) >= 2 and word not in excluded_keywords]\n",
    "    split_sentence = ' '.join(split)\n",
    "    custom_feat =[\n",
    "        1 if any(tld in url_element for tld in nl_tlds) else 0,\n",
    "        1 if any(tld in url_element for tld in other_tlds) else 0\n",
    "    ]\n",
    "    urls_tokenized.append([split_sentence, custom_feat, url_element, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tevansleen nl', [1, 0], 'https://www.tevansleen.nl/', 1]\n"
     ]
    }
   ],
   "source": [
    "print(urls_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: NOT NECESSARY SINCE TRAIN_TEST_SPLIT ALREADY SHUFFLES VALUES\n",
    "\n",
    "#Train Test split - ONLY RUN ONCE AFTER INITIALIZATION\n",
    "# random.shuffle(urls_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tevansleen nl 1\n",
      "  (0, 1179)\t1\n",
      "  (0, 2909)\t1\n",
      "  (0, 2476)\t1\n",
      "  (0, 1722)\t1\n",
      "  (1, 2909)\t1\n",
      "  (1, 1722)\t1\n",
      "  (1, 1178)\t1\n",
      "  (1, 595)\t1\n",
      "  (2, 1179)\t1\n",
      "  (2, 1722)\t1\n",
      "  (2, 1189)\t1\n",
      "  (3, 1179)\t1\n",
      "  (3, 2909)\t1\n",
      "  (3, 1722)\t1\n",
      "  (3, 2870)\t1\n",
      "  (3, 2299)\t1\n",
      "  (4, 1179)\t1\n",
      "  (4, 2909)\t1\n",
      "  (4, 1722)\t1\n",
      "  (4, 2426)\t1\n",
      "  (5, 1179)\t1\n",
      "  (5, 2909)\t1\n",
      "  (5, 1722)\t1\n",
      "  (5, 2547)\t1\n",
      "  (6, 1179)\t1\n",
      "  :\t:\n",
      "  (1998, 1179)\t1\n",
      "  (1998, 2909)\t1\n",
      "  (1998, 594)\t1\n",
      "  (1998, 1176)\t1\n",
      "  (1998, 2127)\t1\n",
      "  (1998, 1652)\t1\n",
      "  (1998, 1799)\t1\n",
      "  (1998, 27)\t1\n",
      "  (1998, 911)\t1\n",
      "  (1998, 1821)\t1\n",
      "  (1998, 38)\t1\n",
      "  (1998, 11)\t1\n",
      "  (1998, 2274)\t1\n",
      "  (1998, 978)\t1\n",
      "  (1998, 227)\t1\n",
      "  (1998, 2624)\t1\n",
      "  (1998, 1250)\t1\n",
      "  (1998, 2688)\t1\n",
      "  (1998, 1075)\t1\n",
      "  (1999, 1179)\t1\n",
      "  (1999, 2909)\t1\n",
      "  (1999, 594)\t1\n",
      "  (1999, 912)\t1\n",
      "  (1999, 674)\t1\n",
      "  (1999, 116)\t1\n"
     ]
    }
   ],
   "source": [
    "X_tokens, X_url, y = [], [], []\n",
    "for url in urls_tokenized:\n",
    "    X_tokens.append(url[0])\n",
    "    X_url.append(url[2])\n",
    "    y.append(url[3])\n",
    "\n",
    "print(X_tokens[0], y[0])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_tokens = vectorizer.fit_transform(X_url)\n",
    "\n",
    "print(X_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000054961\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out()[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_token_train, X_token_test, y_token_train, y_token_test = train_test_split(X_tokens, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# ccTLD - no training necessary because it just looks at TLD:\n",
    "ccTLD_y = []\n",
    "for url in X_url:\n",
    "    ccTLD_y.append(1 if any(tld in url for tld in nl_tlds) else 0)\n",
    "\n",
    "print(len(ccTLD_y))\n",
    "\n",
    "# ccTLD+ Would give the same outcome as TLD in our case because it counts generic TLD's (.com/.net/.org) as non-dutch anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1179)\t1\n",
      "  (0, 2909)\t1\n",
      "  (0, 366)\t1\n",
      "  (0, 1353)\t1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_token_train[0])\n",
    "print(y_token_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token features\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_token_train.toarray(), y_token_train).predict(X_token_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1000,    0],\n",
       "       [ 209,  791]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, ccTLD_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0\n",
      " 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
      " 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 88, 108],\n",
       "       [  2, 202]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_token_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without numeric characters: \n",
    "array([[ 94, 106],\n",
    "       [  3, 197]])\n",
    "\n",
    "With numeric characters:\n",
    "array([[ 88, 108],\n",
    "       [  2, 202]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
