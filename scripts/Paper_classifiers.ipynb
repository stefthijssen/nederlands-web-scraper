{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_f = open('../data/nl.json')\n",
    "en_f = open('../data/en.json')\n",
    "nl_data = json.load(nl_f)\n",
    "en_data = json.load(en_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_tokenized = []\n",
    "excluded_keywords = ['www', 'index', 'html', 'htm', 'html', 'http', 'https']\n",
    "nl_tlds = ['.nl/', '.be/', '.su/', '.aw/', '.sx/', '.cw/']\n",
    "other_tlds = ['.com/', '.net/', '.org/']\n",
    "\n",
    "for element in nl_data[0:5000]:\n",
    "    url_element = element['siteUrl'] + ('' if element['siteUrl'].endswith('/') else '/')\n",
    "    split = [word for word in re.split('[^a-zA-Z]', url_element) \n",
    "        if len(word) >= 2 and word not in excluded_keywords]\n",
    "    split_sentence = ' '.join(split)\n",
    "    custom_feat =[\n",
    "        1 if any(tld in url_element for tld in nl_tlds) else 0,\n",
    "        1 if any(tld in url_element for tld in other_tlds) else 0\n",
    "    ]\n",
    "    urls_tokenized.append([split_sentence, custom_feat, url_element, 1])\n",
    "\n",
    "for element in en_data[0:5000]:\n",
    "    url_element = element['siteUrl'] + ('' if element['siteUrl'].endswith('/') else '/')\n",
    "    split = [word for word in re.split('[^a-zA-Z]', url_element) \n",
    "        if len(word) >= 2 and word not in excluded_keywords]\n",
    "    split_sentence = ' '.join(split)\n",
    "    custom_feat =[\n",
    "        1 if any(tld in url_element for tld in nl_tlds) else 0,\n",
    "        1 if any(tld in url_element for tld in other_tlds) else 0\n",
    "    ]\n",
    "    urls_tokenized.append([split_sentence, custom_feat, url_element, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tevansleen nl', [1, 0], 'https://www.tevansleen.nl/', 1]\n"
     ]
    }
   ],
   "source": [
    "print(urls_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: NOT NECESSARY SINCE TRAIN_TEST_SPLIT ALREADY SHUFFLES VALUES\n",
    "\n",
    "#Train Test split - ONLY RUN ONCE AFTER INITIALIZATION\n",
    "# random.shuffle(urls_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tevansleen nl 1\n",
      "  (0, 10558)\t1\n",
      "  (0, 7549)\t1\n",
      "  (1, 7549)\t1\n",
      "  (1, 2194)\t1\n",
      "  (2, 7549)\t1\n",
      "  (2, 5052)\t1\n",
      "  (3, 7549)\t1\n",
      "  (3, 12101)\t1\n",
      "  (3, 9710)\t1\n",
      "  (4, 7549)\t1\n",
      "  (4, 10274)\t1\n",
      "  (5, 7549)\t1\n",
      "  (5, 10851)\t1\n",
      "  (6, 7549)\t1\n",
      "  (6, 10551)\t1\n",
      "  (7, 894)\t1\n",
      "  (7, 2193)\t1\n",
      "  (8, 7549)\t1\n",
      "  (8, 1420)\t1\n",
      "  (9, 2193)\t1\n",
      "  (9, 9813)\t1\n",
      "  (10, 7549)\t1\n",
      "  (10, 9998)\t1\n",
      "  (10, 5152)\t1\n",
      "  (10, 4762)\t1\n",
      "  :\t:\n",
      "  (9992, 963)\t1\n",
      "  (9993, 2193)\t1\n",
      "  (9993, 8362)\t1\n",
      "  (9993, 11885)\t1\n",
      "  (9994, 2193)\t1\n",
      "  (9994, 433)\t1\n",
      "  (9994, 7781)\t1\n",
      "  (9994, 2603)\t1\n",
      "  (9995, 2193)\t1\n",
      "  (9995, 1314)\t1\n",
      "  (9995, 784)\t1\n",
      "  (9996, 2193)\t1\n",
      "  (9996, 10912)\t1\n",
      "  (9996, 5642)\t1\n",
      "  (9996, 5641)\t1\n",
      "  (9997, 2193)\t2\n",
      "  (9997, 3033)\t1\n",
      "  (9997, 5013)\t1\n",
      "  (9997, 8589)\t1\n",
      "  (9997, 1875)\t1\n",
      "  (9997, 9281)\t1\n",
      "  (9998, 3961)\t1\n",
      "  (9998, 8426)\t1\n",
      "  (9999, 2193)\t1\n",
      "  (9999, 12273)\t1\n"
     ]
    }
   ],
   "source": [
    "X_tokens, X_url, y = [], [], []\n",
    "for url in urls_tokenized:\n",
    "    X_tokens.append(url[0])\n",
    "    X_url.append(url[2])\n",
    "    y.append(url[3])\n",
    "\n",
    "print(X_tokens[0], y[0])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_tokens = vectorizer.fit_transform(X_tokens)\n",
    "\n",
    "print(X_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actcult\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out()[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_token_train, X_token_test, y_token_train, y_token_test = train_test_split(X_tokens, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# ccTLD - no training necessary because it just looks at TLD:\n",
    "ccTLD_y = []\n",
    "for url in X_url:\n",
    "    ccTLD_y.append(1 if any(tld in url for tld in nl_tlds) else 0)\n",
    "\n",
    "print(len(ccTLD_y))\n",
    "\n",
    "# ccTLD+ Would give the same outcome as TLD in our case because it counts generic TLD's (.com/.net/.org) as non-dutch anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 964)\t1\n",
      "  (0, 6032)\t1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_token_train[0])\n",
    "print(y_token_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token features - Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "y_pred_nb = gnb.fit(X_token_train.toarray(), y_token_train).predict(X_token_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token features - Decision Tree\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "y_pred_dt = dtc.fit(X_token_train.toarray(), y_token_train).predict(X_token_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom features\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.9820585457979226\n",
      "recall:  0.832\n",
      "F1:  0.9008228670420095\n"
     ]
    }
   ],
   "source": [
    "mat = confusion_matrix(y, ccTLD_y).ravel()\n",
    "\n",
    "tn, fp, fn, tp = mat.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall: \", recall)\n",
    "print(\"F1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.6523809523809524\n",
      "recall:  0.9755849440488301\n",
      "F1:  0.7818997146351406\n"
     ]
    }
   ],
   "source": [
    "mat = confusion_matrix(y_token_test, y_pred_nb).ravel()\n",
    "\n",
    "tn, fp, fn, tp = mat.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall: \", recall)\n",
    "print(\"F1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[988  29]\n",
      " [119 864]]\n",
      "precision:  0.9675251959686451\n",
      "recall:  0.8789420142421159\n",
      "F1:  0.9211087420042644\n"
     ]
    }
   ],
   "source": [
    "mat = confusion_matrix(y_token_test, y_pred_dt)\n",
    "print(mat)\n",
    "\n",
    "tn, fp, fn, tp = mat.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall: \", recall)\n",
    "print(\"F1: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without numeric characters: \n",
    "array([[ 94, 106],\n",
    "       [  3, 197]])\n",
    "\n",
    "With numeric characters:\n",
    "array([[ 88, 108],\n",
    "       [  2, 202]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
